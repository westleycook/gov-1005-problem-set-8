---
title: "Problem Set 8"
author: "Westley Cook"
date: "4/14/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# standard first load

library(tidyverse)

# for its data

library(fivethirtyeight)

# for skim()

library(skimr)

# for tidy()

library(broom)

# for gt() tables

library(gt)

# loading data

load("raw-data/tweetsnew.Rdata")

```

## Question 1: Exploratory Data Analysis

### 1A) Summary Statistics

```{r question_1a,echo=FALSE}

# This r chunk finds the number of Trump tweets per week and binds that data to
# the fivethirtyeight Trump approval rating data. It then prints a call to the
# skim() function to view summary stats for total tweets and approval rating

# Renaming the approval rating data from fivethirtyeight so it's shorter and
# more wieldy

poll <- trump_approval_poll

# Finding number of tweets per week. Grouping by week and counting, then
# renaming the n column to total_tweets as specified in question prompt

tweets_by_week <- tweets %>% 
  group_by(week) %>% 
  count() %>% 
  rename(total_tweets = n)

# Adding week column to poll data using code from the assignment prompt

poll$week <- ceiling(as.numeric(difftime(poll$end_date, 
                                         "2017-01-01", 
                                         units = "days")
                                )/7)

# Joining tweet data with approval rating poll data replacing all NA tweet
# values with 0 (could have used replace_na() for this instead, but I forgot
# that function existed until after I'd already used mutate() and tested to make
# sure it was working, so I didn't want to change it)

approval_tweets <- poll %>% 
  left_join(tweets_by_week, by = "week") %>% 
  mutate(total_tweets = ifelse(is.na(total_tweets), 0, total_tweets))

# Selecting columns specified in the assignment prompt and printing summary
# stats using skim()

approval_tweets %>% 
  select(total_tweets, approve) %>% 
  skim()


```

### 1B) Bivariate Correlations

```{r question_1b, echo=FALSE}

# This r chunk plots approval rating by total number of tweets and grade of the
# poll. It then finds the correlation coefficient for approval rating and number
# of tweets

# Creating the plot. Use fct_explicit_na() around grade to force it to show
# missing values. Manually set color scheme to viridis and give the legend a
# name (note: not an exact replicate of assignment prompt plot, because I've
# capitalized "Grade" in the legend title, and they didn't - but I think it
# looks better this way). All other text and formatting replicates the
# assignment prompt plot exactly

approval_tweets %>% 
  ggplot(aes(total_tweets, approve, color = fct_explicit_na(grade))) +
  geom_point() +
  theme_classic() +
  labs(title = "Trump Approval Ratings and Number of Tweets",
       subtitle = "Data from fivethirtyeight and Trump Twitter Archive",
       x = "Total Tweets",
       y = "Approval Rating") +
  scale_color_viridis_d(name = "Grade")

# Finding correlation coefficient for approval rating and total tweets, then
# pulling the value and assigning it to an object, which I insert in-line below

cor_coef <- approval_tweets %>% 
  summarize(cor = cor(total_tweets, approve)) %>% 
  pull(cor) %>% 
  round(digits = 3)

```


The correlation coefficient between the approval rating and the number of tweets is **`r cor_coef`**. There does NOT seem to be a strong relationship between tweet activity and approval ratings.

## Question 2: Multivariate Regression

### 2A) Using lm()

```{r question_2a, echo=FALSE}

# This r chunk runs a linear regression of approval ratings on two variables,
# total_tweets and high_q (whether the poll was high quality or not)

# Using mutate() and ifelse() to create the new high_q variable

new_approval_tweets <- approval_tweets %>% 
  mutate(high_q = ifelse(grade %in% c("A+", "A", "A-"),
                         1,
                         0))

# Running a linear regression of approval rating on total_tweets and high_q,
# using tidy() to make the model results easy to work with and conf.int = TRUE
# to grab the upper and lower bounds of a 95% confidence interval. Then
# selecting just the variables of interest and mutating to round them each to a
# few significant digits. Finally, piping them to a gt table and adding a title,
# subtitle, and column labels

new_approval_tweets %>% 
  lm(approve ~ total_tweets + high_q, data = .) %>% 
  tidy(conf.int = TRUE) %>% 
  select(term, estimate, conf.low, conf.high) %>% 
  mutate(estimate = round(estimate, digits = 3),
         conf.low = round(conf.low, digits = 3),
         conf.high = round(conf.high, digits = 4)) %>% 
  gt() %>% 
  tab_header(title = "Effect of Number of Tweets and Poll Quality on Reported 
             Approval Rating",
             subtitle = "Data from fivethirtyeight and Trump Tweet Archive") %>% 
  cols_label(term = "Variable",
             estimate = "Estimate",
             conf.low = "Lower Bound",
             conf.high = "Upper Bound")

```

### 2B) Interpreting Results

The estimated average treatment effect of high_q is -2.347, which means that taking into account all other variables in the model, a high-quality poll is expected to produce an approval rating roughly 2.3 points *lower* than a low-quality poll.

The frequentist interpretation of the confidence interval is that 95% of the time, using the process we used will generate an interval containing the true value of the average treatment effect of each variable on approval rating. The Bayesian interpretation is that there is a 95% chance that the interval we created here contains the true value of the average treatment effect.

### 2C) Interaction Variables

```{r question_2c, echo=FALSE}

# This r chunk runs a new regression that includes total_tweets, high_q, and the
# interaction between total tweets and high quality. It then prints the results
# in a gt table like the one above

# Code here was copied and pasted directly from 2a, with the following edits:

# 1) the lm() call was changed from total_tweets + high_q to total_tweets *
# high_q in order to generate the interaction term

# 2) conf.high was rounded to 3 digits instead of four

new_approval_tweets %>% 
  lm(approve ~ total_tweets * high_q, data = .) %>% 
  tidy(conf.int = TRUE) %>% 
  select(term, estimate, conf.low, conf.high) %>% 
  mutate(estimate = round(estimate, digits = 3),
         conf.low = round(conf.low, digits = 3),
         conf.high = round(conf.high, digits = 3)) %>% 
  gt() %>% 
  tab_header(title = "Effect of Number of Tweets and Poll Quality on Reported 
             Approval Rating",
             subtitle = "Data from fivethirtyeight and Trump Tweet Archive") %>% 
  cols_label(term = "Variable",
             estimate = "Estimate",
             conf.low = "Lower Bound",
             conf.high = "Upper Bound")

```

### 2D) Estimating Fitted Values

This model would predict that for Monmouth University's A+ rated poll during a week in which President Trump tweeted 84 times, the approval rating would be as follows:

Predicted approval rating = Intercept estimate - high_q + (total_tweets + total_tweets:high_q) * number of tweets

Predicted approval rating = 41.629 - 2.701 + (-0.006 + 0.021) * 84 = **40.188**

Calculating the predicted value using R's predict() function, we get almost exactly the same result (discrepancy due to roundoff error):

```{r question_2d}
  
# Naming the interaction model

interaction_model <- lm(approve ~ total_tweets * high_q, 
                        data = new_approval_tweets)

# Predicting the model's result given a high-quality poll and 84 tweets

predict(interaction_model, tibble(total_tweets = 84, high_q = 1))

```

### 2E) Multiple Regression and the Rubin Causal Model

Taking into account all other variables in the model, the coefficient for total_tweets would be the average expected treatment effect for each unit of total_tweets (presumably one tweet) when democrat = 0. In other words, it’s the slope of the line when democrat = 0 (which presumably means the respondent was Republican).

The coefficient for democrat would be the *offset* from the baseline intercept when democrat = 1; the baseline intercept is the expected approval rating if total_tweets = 0 and democrat = 0 (in other words, the expected Republican approval rating if the president doesn’t tweet). So the expected approval rating when democrat = 1 and total_tweets = 0 (or in other words, the expected approval rating for a Democrat when the president doesn’t tweet) would be the baseline intercept *plus* this coefficient.

The coefficient for total_tweets:democrat would be the *offset* from the coefficient for total_tweets when democrat = 1. So the slope of the line when democrat = 1 (presumably meaning the respondent is a Democrat) would be the coefficient for total_tweets *plus* this coefficient.

This seems to be an explanatory model, because it’s attempting to isolate the magnitude of the effect of each variable (total_tweets, democrat, and total_tweets:democrat). If it were a predictive model, it would care more about just getting an accurate prediction than it would about the respective magnitudes of each variable in the model.

## Question 3: Many Regressions

```{r question_3, echo=FALSE}

# This r chunk FILL IN THE GAP HERE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

# Adding new columns for month to both poll data and tweets data, using code
# from the assignment prompt

poll$month <- ceiling(poll$week/4)
tweets$month <- ceiling(tweets$week/4)

# Filtering poll data for the first 11 months and adding high_q column using the
# same code I used in 2a

poll_11m <- poll %>% 
  filter(month %in% 1:11) %>% 
  mutate(high_q = ifelse(grade %in% c("A+", "A", "A-"),
                         1,
                         0))

# Plotting approval rating by poll quality for each month, replicating the top
# right plot in the assignment prompt

poll_11m %>% 
  group_by(month, high_q) %>% 
  summarize(avg_approve = mean(approve)) %>% 
  ggplot(aes(month, avg_approve, color = as.factor(high_q))) +
  geom_line() +
  theme_classic() +
  theme(legend.position = "top") +
  scale_color_manual(name = "Poll Quality",
                     values = c("blue", "red"),
                     labels = c("Lower than A- or missing", "Higher than A-")) +
  labs(title = "Approval Rating by Poll Quality",
       x = "Month",
       y = "Average Approval Rating")

# Counting the number of tweets for each month and filtering for first 11 months
# so it matches the poll_11m data and can be easily joined later

tweets_by_month <- tweets %>% 
  group_by(month) %>% 
  count() %>% 
  rename(total_tweets = n) %>% 
  filter(month %in% 1:11)

# Plotting number of tweets per month, replicating the plot from the bottom
# right corner of the assignment prompt

tweets_by_month %>%
  ggplot(aes(month, total_tweets)) +
  geom_col() +
  theme_classic() +
  labs(title = "Total Tweets",
       subtitle = "President Trump",
       x = "Month",
       y = "Total Tweets")





```

For the left hand side, we recommend following the steps below:

Pipe the data into group_by(month), and then nest(), to create a tibble of tibbles. This tibble will have two columns, “month” and “data”, and 11 rows. Each row of the data column should be a list with all the observations for a given month.

Create a column named mod, using map() to run lm on each row of your tibble. Run the same regression you did in 2A, using total_tweets and high_q as your explanatory variables (no interaction term).

Create a column called reg results, using map() again to run tidy on the results. If you examine these results, you’ll see that for each “term” (in this case, “total_tweets”, “high_q”, and the intercept), it includes the term, the estimate, the standard error, the t statistic, and the p value. We’ll focus on the estimate and the standard error.

Use map_dbl() to create columns for the estimate and standard error for your two variables of interest (total_tweets and high_q, and use those to create an upper and lower column for the 95% confidence interval for each variable. Note that this is an identical process to what you did in PS 7, just with multiple variables.

Make plots of each of the coefficients.

Print your plots here. We like the “patchwork” package for arranging plots.

##### Worked with:
None